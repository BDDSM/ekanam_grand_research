{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/FUlyankin/ekenam_grand_research/master/images/cover.png\">\n",
    "</center>\n",
    "\n",
    "\n",
    "# <center> Иканам гранд рисёрч </center>\n",
    "## <center>  Часть шестая: моделирование </center>\n",
    "\n",
    "\n",
    "Проект **Иканам гранд рисёрч** реализуется [Иканам стьюдентс коммьюнити,](https://vk.com/ikanam)\n",
    "в частности [вот этим парнем по имени Филипп.](https://vk.com/ppilif) Если вы нашли ошибку или у вас есть предложения, замечания, деньги, слава или женщины, можно ему написать. Весь говнокод, использованный в исследовании распространяется по лицензии [Creative Commons CC BY-NC-SA 4.0.](https://creativecommons.org/licenses/by-nc-sa/4.0/) Его можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала. При наличии технической возможности необходимо также указать активную гиперссылку на [страницу рисёрча.](https://github.com/FUlyankin/ekenam_grand_research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Две задачи и одна наука \n",
    "\n",
    "Анализ данных — это довольно обширная область знания. Она включает в себя классический матстат, эконометрику, машинное обучение и многие другие более специфические вещи. Анализ данных занимается тем, что ищет ответ на [два великих вопроса:](https://www.coursera.org/learn/ekonometrika/lecture/S7E9y/1-1-1-sut-mietoda-naimien-shikh-kvadratov) \n",
    "\n",
    "* Как устроен мир? Как переменная $y$ зависит от переменной $x$? \n",
    "* Что будет завтра? Как спрогнозировать переменную $y$? \n",
    "\n",
    "Обычно для поиска ответов на эти два вопроса используются модели. Эти модели оцениваются с помощью собранных данных. На работу каждой модели накладываются какие-то ограничения. Оценивая коэффициенты в моделях, мы хотим, чтобы они обладали тремя свойствами: \n",
    "\n",
    "1. **Несмещенность** — чем больше у нас наблюдений, тем ближе среднее значение наших оценок к истиному значению параметра;\n",
    "2. **Состоятельность** — чем больше данных, тем почти наверное ближе наши оценки к истине;\n",
    "3. **Эффективность** — разброс нашей оценки минимален в некотором классе оценок, если данные меняются, наша оценка меняется менее сильно, нежели другие.\n",
    "\n",
    "В зависимости от того на какой из двух вопросов мы ищем ответ, мы можем приносить некоторые ограничения в жертву. Переформулируем великие вопросы в контексте нашей проблемы и немного подумаем о жертвах, которые мы собираемя принести анализу данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Великий вопрос номер один\n",
    "\n",
    "Интерпретация. Пусть мы хотим знать какие именно переменные увеличивают наши шансы закончить эконом и насколько сильно. \n",
    "В нашем распоряжении оказался довольно большой массив данных. От баллов за ЕГЭ до подписок и информации из профилей. Предположим, что мы оценили модель только по баллам ЕГЭ и по наличию олимпиады. Коэффицент перед переменной, отвечающей за наличие олимпиады получился довольно большим. Он равен $42$. \n",
    "\n",
    "Означает ли это, что при наличии олимпиады, мои шансы закончить эконом возрастают пропорционально этому коэффициенту?  Заметьте, я не говорю, что вероятность закончить эконом увеличивается на 42 процентных пункта, потому что мы оцениваем логистическую регрессию. В такой модели интерпретация коэффициентов устроена более сложным образом. Значение коэффициента $\\hat \\beta$ интерпретируется, как величина изменения шансов (смотри 2 том Носко, страницу 205) либо [7 неделю лекций Бориса Борисовича,](https://www.coursera.org/learn/ekonometrika/lecture/JaRpY/7-1-5-loghit-modiel-doska) а лучше и то и то.\n",
    "\n",
    "Так вот, означат ли это, что мои шансы закончить эконом растут пропорционально 42? Нет, не означает. Почему? Потому что вероятность закончить первый курс, в равной степени как и способность написать олимпиаду или хорошо сдать ЕГЭ, зависят от такой ненаблюдаемой переменной, как уровень умственных способностей. Эта переменная, в нашей ситуации оказалась в ошибке. В конечном счёте, из-за её пропуска возникает корреляция регрессора и ошибки, оценки коэффициентов оказываются смещёнными и несостоятельными, а сама по себе эта ситуация, называется **эндогенностью.** В этом месте я хотел бы передать привет третьему курсу, который, по большей части, не мог сформулировать мне на зачёте по эконометрике, определение эндогенности. \n",
    "\n",
    "Так вот, из-за эндогенности, оценки коэффициентов оказываются **смещенными** и **несостоятельными**. Они ни в коей мере не отражают реальность. В связи с этим мы ничерта не можем сказать о влиянии олимпиады на шансы закончить эконом. Для того, чтобы сделать это, нужно добавить в модель какие-то **инструментальные переменные**, которые будут нести в себе информацию об уровне умственных способностей человека. \n",
    "\n",
    "В нашем случае, за создание таких переменных отвечает социальная сеть. Мы наблюдаем за тем, на какие паблики подписывается человек, что он пишет у себя в профиле и так далее. Это позволяет собрать какую-то информацию о его складе ума, характере и других, одному процессу порождения данных ведомых, скрытых от нас характеристиках. В конечном счёте, когда мы добавим в модель эти переменные, мы вынесем ненаблюдаемый эффект из ошибки, корреляция между ошибкой и регрессором исчезнет и оценки в модели получатся состоятельными. \n",
    "\n",
    "Предположим, что оценка коэффициента после всех этих манипуляций, оказалась равна $5$. Если никаких других источников эндогенности не осталось,и, при этом, объём данных был довольно большим, мы, дейтвительно можем сказать, что наличие олимпиды увеличивает шансы человека закончить эконом пропроционально 5. Конечно же, при условии, если коэффициент оказывается значим. \n",
    "\n",
    "Мы боролись за интерпретацию коэффициента перед переменной, отвечающей за олимпиаду и хотели узнать насколько это важно. Можно сказать, что наша борьба закончилась успешно. \n",
    "\n",
    "**Но ведь кроме олимпиады у нас в модели куча других переменных!** Например,  в ней есть переменная, которая отвечает за наличие подписки на мдк. И она равна $-3000$. Означает ли это, что если я прямо сейчас отпишусь от мдк, мои шансы закончить эконом так сильно вырастут? \n",
    "\n",
    "Если вы знали об исследовании и отписались из-за этого, **ничего у вас не вырсатет,** потому что изменилась не причина, а следствие.  Если вы не знали об исследовании, в вашей жизни произошло какое-то переосмысление ценностей, вы поняли, что там тупые мемы, а ботать очень круто, то да. Ваши шансы закончить эконом возрастут. Возрастут не потому что вы отписались от мдк, а потому что что-то произошло с причиной того, что вы на него были подписаны. Произошло что-то с ненаблюдаемой переменной, которая несёт информацию о вашем характере.\n",
    "\n",
    "**Ещё раз, ещё раз,** в первой ситуации, просто-напросто, испортился информационный канал, за который отвечала инструментальная переменная \"паблик мдк\", во втором случае изменилась истиная информация и наш информационный канал в виде инструментальной переменной сообшил нам про это.  Интерпретировать коэффициенты в логистической регрессии можно так и только так. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Великий вопрос номер два \n",
    "\n",
    "Прогнозы. Пусть мы хотим уметь хорошо прогнозировать отчислят ли человека с эконома или нет. Предположим, что мы оценили регрессию из предыдущего пункта. Она хороша, коэффициенты хороши, всё значимо, на улице светит солнце, все гиптезы проверены, вода в ручьях кристально чистая, эндогенности нет и во всём мире мир. Можно ли использовать её для прогнозирования? **Конечно да.** Ура, расходимся!\n",
    "\n",
    "**Секундочку. А что если я скажу вам, что ваши прогнозы можно улучшить?** При оценивании логистической регрессии максимизируется логарифм правдоподобия. Этот логарифм можно немного переписать и получить логистическую функцию потерь, которую уже нужно будет минимизировать. Уже к ней можно присобачить дополнительное слагаемое, которое будет отвечать за ограничения, накладываеммые на коэффициенты. Если выбрать перед этим ограничением правильный множатель и проминимизировать всё это добро, то коэффициенты окажутся смещенными, но при этом прогнозы станут лучше. \n",
    "\n",
    "Как такое происходит? А очень просто! [Можно показать,](https://habrahabr.ru/company/ods/blog/323890/) что ошибка прогноза складывается из **квадрата смещения**, **дисперсии** и **неустранимой ошибки**\n",
    "\n",
    "$$\n",
    "Err = Var(\\hat y) + Bias^2(\\hat y) + \\sigma^2 \n",
    "$$\n",
    "\n",
    "\n",
    "Если с последней мы ничего сделать не можем, то на первые два слагаемых мы можем как-то влиять. В идеале, конечно же, хотелось бы свести на нет оба этих слагаемых (левый верхний квадрат рисунка), но на практике часто приходится балансировать между смещенными и нестабильными оценками (высокая дисперсия).\n",
    "\n",
    "<img align=\"center\" src=\"https://hsto.org/files/aa5/d0f/149/aa5d0f149838470fb997ca405c4c55a0.png\" width=\"350\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правило, при увеличении сложности модели (например, при увеличении количества свободных параметров) увеличивается дисперсия (разброс) оценки, но уменьшается смещение. Из-за того что тренировочный набор данных полностью запоминается вместо обобщения, небольшие изменения приводят к неожиданным результатам (переобучение). Если же модель слабая, то она не состоянии выучить закономерность, в результате выучивается что-то другое, смещенное относительно правильного решения.\n",
    "\n",
    "Наша святая всех святых, **Теорема Гаусса-Маркова** как раз утверждает, что МНК-оценка параметров линейной модели является самой лучшей в классе несмещенных линейных оценок, то есть с наименьшей дисперсией. Вопрос в предпосылках теоремы Гаусса-Маркова. Они практически никогда не выполнены. Это позволяет регуляризатором уменьшить разброс за счёт смещения и улучшить прогнозы. \n",
    "\n",
    "Однако, в этом случае, мы приносим в жертву интерпретацию. Оценки коэффициентов оказываются смещёнными и не отражают никаких реалий. Единственное, на что в такой ситуации можно обращать внимание, это знак коэффициента и его размер. Чем больше абсолютное значение коэффициента, тем более важна эта переменная. Но какой именно вклад она вносит, мы не знаем. \n",
    "\n",
    "Более подробно, со всеми формулами и выводами, об этом (и не только об этом) можно почитать в двух отлично сделанных статьях на хабре. \n",
    "\n",
    "1. [Про переобучение, комплексные собственные значения, регуляризаторы и байесовскую интерпретацию линейных моделей](https://habrahabr.ru/company/ods/blog/322076/)\n",
    "2. [Про линейные модели и логит.](https://habrahabr.ru/company/ods/blog/323890/) На мой взгляд, написано на стыке эконометрики и машинного обучения. \n",
    "\n",
    "Более того, модели можно усложнять. В большей части ситуаций, такое усложнение, делает модель неинтерпретируемой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 О том, где находится тонкая грань\n",
    "\n",
    "Итак, есть два великих вопроса. **А ещё есть совершенно дебильная грань между людьми, которые пытаются найти ответы на них.** Не просто дебильная грань, а условная дебильная грань. Каждый раз, когда я на неё наталкиваюсь, меня это удивляет и поражает. \n",
    "\n",
    "Область, которая занимается поиском интерпретации, чаще всего называют эконометрикой. Она сконцентрирована на смысле оценок. Она пытается понять за какой рычаг надо дернуть, чтобы целевая переменная изменилась ровно настолько, насколько нам нужно. Эти благородные цели вынуждают эконометриста проверять гипотезы и строить доверительные интервалы, чтобы быть уверенным. Не преведи процесс порождения данных, если что-то пойдёт не так. Прошу заметить, что это не означет, что эконометрика не уделяет прогнозам никакого внимания. Она уделяет им огромное внимание (вспомним хотябы то, как изучаются временные ряды). Но при этом это внимание уделяется через призму интерпретации. Чаще всего специалистами по эконометрике становятся экономисты. Главная отличительная особенность эконометристов в том, что они относятся к данным как к чему-то очень хрупкому. Они бояться их исказить. Эконометрика является более академичной. Она довольно стара.  \n",
    "\n",
    "Область, которая концентрируется на прогнозах, чаще всего называют машинным обучением. Эта область использует ровно те же самые концепции, что и эконометрика, но немного иначе. Концентрация на прогнозах обогащает арсенал статитсики всякими приблудами вроде нейронок и деревьев. Интерпретируемости моделей уделяется второстепенная роль. На неё смотрят сквозь призму прогнозирования. Чаще всего специалистами по машинному обучению становятся прогеры. Главная отличительная особенность таких людей в том, что они смотрят на машинное обучение как на набор алгоритмов и относятся к данным как к расходному материалу. Машинка моложе эконометрики. \n",
    "\n",
    "В итоге, на почве этих различий рождаются [вот такие статьи.](http://www.machinelearning.ru/wiki/images/8/85/Breiman01stat-ml.pdf) и [записи в блогах по ним.](http://econometricsense.blogspot.ru/2011/01/classical-statistics-vs-machine.html) Вот как так получается, чёрт возьми! Я экономист. Сначала я выучил эконометрику, затем машинку. Я очень удивился тому, что на эконометрике никто не обмолвился ни одним словом про кросс-валидацию и регуляризацию. Как можно вообще изучать временные ряды, где одной из целей является именно прогнозирование, без кросс-валидации? Ещё больше я удивляюсь, когда пацаны с ВМК занимаются машинкой и не знают что такое гетероскедастичность или инструментальная переменная. В этом же блоге есть вот такая замечательная картинка. \n",
    "\n",
    "<img align=\"center\" src=\"http://3.bp.blogspot.com/_0Lnn2oP30gU/TTWr0XRM1MI/AAAAAAAAAKk/pqA6JSThqsA/s320/data+science+Venn-724969.jpeg\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы закончили ВМК? Умеете прогать и можете в матан? Вы машинлёрнер! Вы закончили эконом и можете в здравый смысл и всякие разные эконом-конецепции, а ещё вы можете в матан, потому что вы закончили нормальный эконом? Вы традиционый рисёрчер-эконометрист! Ну не херня ли? Боже мой, а когда речь заходит о сраче между python, основным инструментов машинлёрнеров и R, основным инструментов эконометристов, то тут хоть стой хоть падай. Оба языка прекрасны! Внешне они похожи, поэтому их легко осваивать. При этом они сделаны под разные задачи. Например, я никогда не буду заниматься временными рядами в python, точно также как я не буду никогда заниматься нейросетями в R. \n",
    "\n",
    "УХ! Вместо кратенького введния в моделирование получился целый толмуд философии. Не ожидал я от себя такого. Друзья! Я призываю вас ещё разок посмотреть на диаграмму выше. Не надо быть традиционным рисёрчером-эконометристом. Не надо быть машинлёрнером. Обе этих дороги одни сплошные ограничения. Смотрите на всё это добро как на одну огромную цельную область. Бутстрапьте доверительные интервалы для деревьев, кросс-валидируйте вары и векмы. Будьте дата-сайнтистами. Это окроет перед вами огромные приемущества.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Классификация\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Пока наш алгоритм выдавал оценки принадлежности к классу 1. Ясно, что на практике нам часто надо будет решить: какие объекты отнести к классу 1, а какие к классу 0. Для этого нужно будет выбрать некоторый порог (объекты с оценками выше порога считаем принадлежащими классу 1, остальные – 0).\n",
    "\n",
    "Выбору порога соответствует выбор точки на ROC-кривой.\n",
    "\n",
    "Заметим, что точка по абсциссе – это процент точек класса 0, которые неверно классифицированы нашим алгоритмом (это называется FPR = False Positive Rate). Заметим, что точка по ординате – процент точек класса 1, которые верно классифицированы нашим алгоритмом (это называется TPR = True Positive Rate). Именно в этих координатах (FPR, TPR) построена ROC-кривая. Часто в литературе её определяют как кривую зависимости TPR от FPR при варьировании порога для бинаризации.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "После осознания того, какую именно задачу требуется решить на этих данных, следующим шагом при реальном анализе был бы подбор подходящего метода. В данном задании выбор метода было произведён за вас, это логистическая регрессия. Кратко напомним вам используемую модель.\n",
    "\n",
    "Логистическая регрессия предсказывает вероятности принадлежности объекта к каждому классу. Сумма ответов логистической регрессии на одном объекте для всех классов равна единице.\n",
    "\n",
    "$$ \\sum_{k=1}^K \\pi_{ik} = 1, \\quad \\pi_k \\equiv P\\,(y_i = k \\mid x_i, \\theta), $$\n",
    "\n",
    "где:\n",
    "- $\\pi_{ik}$ - вероятность принадлежности объекта $x_i$ из выборки $X$ к классу $k$\n",
    "- $\\theta$ - внутренние параметры алгоритма, которые настраиваются в процессе обучения, в случае логистической регрессии - $w, b$\n",
    "\n",
    "Из этого свойства модели в случае бинарной классификации требуется вычислить лишь вероятность принадлежности объекта к одному из классов (вторая вычисляется из условия нормировки вероятностей). Эта вероятность вычисляется, используя логистическую функцию:\n",
    "\n",
    "$$ P\\,(y_i = 1 \\mid x_i, \\theta) = \\frac{1}{1 + \\exp(-w^T x_i-b)} $$\n",
    "\n",
    "Параметры $w$ и $b$ находятся, как решения следующей задачи оптимизации (указаны функционалы с L1 и L2 регуляризацией, с которыми вы познакомились в предыдущих заданиях):\n",
    "\n",
    "L2-regularization:\n",
    "\n",
    "$$ Q(X, y, \\theta) = \\frac{1}{2} w^T w + C \\sum_{i=1}^l \\log ( 1 + \\exp(-y_i (w^T x_i + b ) ) ) \\longrightarrow \\min\\limits_{w,b} $$\n",
    "\n",
    "L1-regularization:\n",
    "\n",
    "$$ Q(X, y, \\theta) = \\sum_{d=1}^D |w_d| + C \\sum_{i=1}^l \\log ( 1 + \\exp(-y_i (w^T x_i + b ) ) ) \\longrightarrow \\min\\limits_{w,b} $$\n",
    "\n",
    "$C$ - это стандартный гиперпараметр модели, который регулирует то, насколько сильно мы позволяем модели подстраиваться под данные.\n",
    "\n",
    "\n",
    "# 2 Прогнозы,  тренировочная и тестовая выборки, кросс-валидация\n",
    "\n",
    "Сконцентрируемся на прогнозах и постраемся как следует обучить модель, которая могла бы предсказать вероятность отчисления с первого курса. \n",
    "\n",
    "## 2.1 \n",
    "\n",
    "**про тест, трэйн и валидацию и метрики **  \n",
    "\n",
    "\n",
    "У нас есть данные за 2012 - 2016 года. Разделим их в следующей пропорции: на данных за 2012 - 2015 (4 года) будем обучать модель. На данных за 2016 год мы будем тестировать модель. \n",
    "\n",
    "## 2.2 \n",
    "\n",
    "**про самые херовые модели** \n",
    "\n",
    "Обратим внимание на то, что в нашей выборке \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Регрессия - моя профессия \n",
    "\n",
    "Итак, наша основная надежда при таком маленьком объёме данных - линейные модели. Попробуем идти по следующему плану.\n",
    "\n",
    "1. Проскалируем реальные фичи. Категориальные либо выбросим либо превратим в dummy. \n",
    "1. Строим модели отдельно по приказам, стенам, фоткам и пабликам. Смотрим на их прогнозную силу. Отбор переменных делаем с помощью Lasso-регрессии. Смотрим на то как опасны переменные из будущего. \n",
    "2. Строим модель по приказам + профилям, приказам + пабликам и тд, смотрим что из этого выйдет. На этом и всех дальнейших этапах мы будем отбирать фичи по двум стратегиям: Lasso и последовательные тесты на совместную значимость.\n",
    "3. Строим модель по всему вместе. \n",
    "4. Моя интуиция подсказывает мне, что фотки и стены не дадут никакого профита для наших моделей. Попробуем построить модель без фоток и стен. \n",
    "5. Все модели выше мы будем оценивать на выборке 2012-2015. Оценивать их качество и сравнивать их между собой мы будем на 2016 году. В качестве метры качества модели мы будем использовать ROC-AUC, который нечувствителен к выбору порога. \n",
    "6. Для модели, которая обладает наилучшей прогнозной силой мы выберем самое няшное значение порога, спрогнозируем вероятности и отчисления. \n",
    "7. Ради фана попробуем построить деревья и небольшую нейросетку. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Своя модель для каждого датасета и опасности будущего \n",
    "\n",
    "\n",
    "\n",
    "| Даныне     | Переменных до  | Переменных после |Roc-Auc_val |Roc-Auc_test|\n",
    "|------------|----------------|------------------|------------|------------|\n",
    "| Приказы    |\n",
    "| + ходит    |\n",
    "| Профиль    |  \n",
    "| +ходит     | \n",
    "| Фотки      |\n",
    "| Стена      |\n",
    "| Паблики 20 |\n",
    "| Паблики 30 |\n",
    "\n",
    "\n",
    "\n",
    "Все переменные, которые пришли к нам из будущего мы удалим и больше не будем использовать при дальнейшем моделировании. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Модели по разным комбинациям\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Хочу всё и сразу \n",
    "\n",
    "Построим модель по всему. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Модель без стен и фоток\n",
    "\n",
    "Построим модель без стен и фоток "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Находим лучшее по соотношению интерпретация-прогнозы \n",
    "\n",
    "\n",
    "\n",
    "Обязательно написать про переобучение под 2016 год и тп.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Прогнозы и выбор порога\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Нелинейные модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ансамбль из моделей "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Итоги "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Регрессия - моя профессия \n",
    "\n",
    "Начнём с самой простой модели. С логистической регрессии. Для начала построим несколько моделей по различны, отдельным датасетам и посмотрим на их прогнозную силу. Смотреть будем на Roc-Auc.\n",
    "\n",
    "\n",
    "| Даныне     | Переменных до  | Переменных после |Roc-Auc |Accuracy |\n",
    "|------------|----------------|------------------|--------|---------|\n",
    "| Приказы    |  8             |  0               |  -     | -       |\n",
    "| + ходит    |  10            |  2               | 0.86   | 0.84    |\n",
    "| Профиль    |  38            |  33              | 0.79   | 0.77    |\n",
    "| Стена      |  108           |  15              | 0.58   | 0.6     |\n",
    "| Фотки      |  16            |  6               | 0.24   | 0.33    | \n",
    "| Паблики 20 |  20            |  3               | 0.64   | 0.52    | \n",
    "| Паблики 30 |  30            |  7               | 0.62   | 0.5     | \n",
    "\n",
    "1. При оценивании модели только по приказам, все переменные зануляются. \n",
    "2. Когда мы добавляем в модель субективные переменные, отвечающие за посещение пар и тус, качество модели резко подскакивает. Не зануляются только новые две переменные. \n",
    "\n",
    "\n",
    "Начинаем смешивать данные. \n",
    "\n",
    "| Даныне     | Переменных до  | Переменных после |Roc-Auc |Accuracy |\n",
    "|------------|----------------|------------------|--------|---------|\n",
    "| П + Пф     |  46            |  17              |  0.89  | 0.9     |\n",
    "| П +Пф+Па30 |  76            |  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Даныне   | Переменных| Accuracy | Roc-Auc |   |\n",
    "|----------|-----------|----------|---------|---|\n",
    "| Приказы  |    8-2    |   0.72   |  0.69   |   |\n",
    "| + ходит  |   10-4    |   0.83   |  0.84   |   |\n",
    "| + профиль|   48-20   |   0.84   |  0.92   |   |\n",
    "| - ходит  |   46-17   |   0.80   |  0.90   |   |\n",
    "| + фотки  |   62-25   |   0.81   |  0.86   |   |\n",
    "| + стены  |  165-33   |   0.78   |  0.82   |   |\n",
    " \n",
    "\n",
    "1. При оценивании модели только по приказам, все переменные, которые отвечают за целевиков, льготы и так далее, занулились. Остались только две переменные с баллами ЕГЭ. Это довольно эндогенный результат. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| имя          |  вероятность | что было |\n",
    "|--------------|--------------|---------|\n",
    "|Куркин Николай|0.995294895617|1.0|\n",
    "|Скибенко Светлана|0.952542654364|0.0|\n",
    "|Дзюба Артём|0.908269342469|0.0|\n",
    "|Радионова Елена|0.907303030059|1.0|\n",
    "|Морозова Виолетта|0.828651849516|1.0|\n",
    "|Петайкина Анастасия|0.811467646196|1.0|\n",
    "|Селищева Анна|0.749912094533|1.0|\n",
    "|Ахмедова Вазифа|0.740710157971|1.0|\n",
    "|Говоркова Дарья|0.70569394833|1.0|\n",
    "|Лушина Валерия|0.691604143451|1.0|\n",
    "|Ваганова Даша|0.68076279012|0.0|\n",
    "|Кириленко Светлана|0.680323857177|1.0|\n",
    "|Кусегенов Данислам|0.653908790702|1.0|\n",
    "|Коннова Анастасия|0.644749281522|1.0|\n",
    "|Корчагин Максим|0.637929241137|1.0|\n",
    "|Рединская Наталья|0.63007258892|1.0|\n",
    "|Даминова Рита|0.618193670932|1.0|\n",
    "|Харитонова Марина|0.615591791545|1.0|\n",
    "|Сидоров Александр|0.608708457649|1.0|\n",
    "|Гаджиева Аида|0.602301687071|1.0|\n",
    "|Дугарова Александра|0.590847076144|1.0|\n",
    "|Аксенова Анна|0.585531140205|1.0|\n",
    "|Буханов Денис|0.57556133613|1.0|\n",
    "|Ахметова Зиля|0.574872340803|1.0|\n",
    "|Потапова Мария|0.574863278783|1.0|\n",
    "|Ломоносов Даниил|0.570473809987|1.0|\n",
    "|Семёнова Алёна|0.570115987797|1.0|\n",
    "|Китькова Ирина|0.563340770062|1.0|\n",
    "|Корнилов Валера|0.554974135541|0.0|\n",
    "|Жуковская Оксана|0.552301773744|1.0|\n",
    "|Комов Михаил|0.536372248361|1.0|\n",
    "|Мидюкин Максим|0.534313798311|1.0|\n",
    "|Зайцева Анна|0.530561599348|1.0|\n",
    "|Зиновьева Анастасия|0.525254749468|1.0|\n",
    "|Сакович Дарья|0.521886083986|1.0|\n",
    "|Ипатьев Кирилл|0.507131966083|1.0|\n",
    "|Руч Камила|0.505822003473|1.0|\n",
    "|Воеводина Алена|0.504285106633|0.0|\n",
    "|Челядинова Екатерина|0.503883446265|1.0|\n",
    "|Аскольская Надежда|0.501404865626|1.0|\n",
    "|Лупарев Евгений|0.500881702821|1.0|\n",
    "|Добрынина Екатерина|0.498527500701|1.0|\n",
    "|Ефремова Ольга|0.489077937902|1.0|\n",
    "|Соломник Михаил|0.480015946351|0.0|\n",
    "|Ахметзянова Дарья|0.47976918341|0.0|\n",
    "|Горячева Олеся|0.475724110442|0.0|\n",
    "|Королькевич Глеб|0.472063824847|1.0|\n",
    "|Филимонова Мария|0.471010427799|0.0|\n",
    "|Шакиров Роман|0.460489072532|1.0|\n",
    "|Завизион Мария|0.45977058626|0.0|\n",
    "|Щербаков Александр|0.453659788956|1.0|\n",
    "|Гвоздева Елизавета|0.448851487779|1.0|\n",
    "|Ядав Лолита|0.446551352017|1.0|\n",
    "|Самойлова Катя|0.438006243015|1.0|\n",
    "|Хуснутдинова Валерия|0.431050942094|1.0|\n",
    "|Горила Феличия|0.426881438193|1.0|\n",
    "|Решетов Владислав|0.403018198205|1.0|\n",
    "|Пихеева Валерия|0.369744081679|1.0|\n",
    "|Хачатрян Гарик|0.360161728007|0.0|\n",
    "|Васильченко Алина|0.358112316071|1.0|\n",
    "|Полякова Александра|0.321964030357|0.0|\n",
    "|Шкредов Михаил|0.317226372592|0.0|\n",
    "|Муцольгова Зарема|0.30697156636|0.0|\n",
    "|Гулиева Эсет|0.294619347838|0.0|\n",
    "|Казарян Диана|0.288074422208|0.0|\n",
    "|Миронов Пётр|0.284037864027|0.0|\n",
    "|Самотканова Виктория|0.267507449648|1.0|\n",
    "|Абасин Хасан|0.267311486043|0.0|\n",
    "|Гулиева Залина|0.233131312084|0.0|\n",
    "|Зыкин Иван|0.213889601538|0.0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ансамбли шутку про трансформеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
